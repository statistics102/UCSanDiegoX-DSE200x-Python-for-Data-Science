{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Assignment: Python Word Count (Optional) \n",
    "## Word Cloud\n",
    "\n",
    "You've likely seen word-clouds before, if not, please check here for examples. In order to create word clouds, the software finds the most frequently occurring words in a text file. Our mini-programming assignment will ask you to do just that. We'll use the text of the famous novel by Charles Dickens, A Tale of Two Cities, in our example, but you can use any text you'd like. \n",
    "\n",
    "### Part 1: Writing and running python code\n",
    "\n",
    "1. Make sure you have the environment for the course already setup.  If not, please see the instructions at the end of Week 1.\n",
    "\n",
    "2. Feel free to create a python program using a text editor (nano) or you can do all the work in python shell.  Or, if you already have some experience in Jupyter, feel free to do your work there instead. We don't want to be prescriptive here, however you'd like to get started programming is fine. We'll be working in Jupyter from this week on, so this assignment is just to get a little practice in python programming.\n",
    "\n",
    "### Part 2: Grab source files\n",
    "Download the source files [here](https://prod-edxapp.edx-cdn.org/assets/courseware/v1/b373c297be36d3ac1cc9452cfa3807e8/asset-v1:UCSanDiegoX+DSE200x+1T2018+type@asset+block/word_cloud.zip)\n",
    "\n",
    "Included in the source files are:\n",
    "\n",
    "1. word_cloud.py <- Starter file if you wish to use it\n",
    "\n",
    "2. 98-0.txt <- Tale of Two Cities, by Charles Dickens. Credit to Project Gutenberg.\n",
    "\n",
    "3. stopwords <- common words to exclude. Credit to Andreas Mueller. \n",
    "\n",
    "Note that we could use the nltk stopwords instead of those provided. You should feel free to do so if you wish.\n",
    "\n",
    "### Part 3: Word Count\n",
    "\n",
    "To complete this assignment, you will want to read and clean the input, then count the frequencies of each word. Remember that the data science process involves some pre-processing, then consists of some analysis itself. \n",
    "\n",
    "Optionally, you can also filter out common words (“the”, “this”, “and”, etc.) by excluding words which appear in the stopwords file.\n",
    "\n",
    "Overall, your approach will be:\n",
    "\n",
    "- Create a data structure to store the words and the number of occurrences of the word.\n",
    "\n",
    "- Read in each word from the file, making it lower case and removing punctuation. (Optionally, skip common words).\n",
    "\n",
    "          For each remaining word, add the word to the data structure or update your count for the word\n",
    "\n",
    "- Extract the top ten most frequently occurring words from your data structure and print them, along with their frequencies.\n",
    "\n",
    "Checking your solution:\n",
    "\n",
    "You will get different counts on words depending on what punctuation you remove, what stop words you use, etc.  So don't worry too much about getting the exact count we have.  But if you want to see what we found, here are two examples:\n",
    "\n",
    "Without using stop words and removing the punctuation (. , \" “ ), the top 10 most common words should be:\n",
    "\n",
    "the : 8177 \n",
    "\n",
    "and : 4984 \n",
    "\n",
    "of : 4122 \n",
    "\n",
    "to : 3536 \n",
    "\n",
    "a : 2976 \n",
    "\n",
    "in : 2612 \n",
    "\n",
    "his : 1998 \n",
    "\n",
    "it : 1879 \n",
    "\n",
    "i : 1872 \n",
    "\n",
    "that : 1861\n",
    "\n",
    "Using the stop words and removing the punctuation (. , \" “ ), the top 10 most common words should be:\n",
    "\n",
    "said : 642 \n",
    "\n",
    "mr : 616 \n",
    "\n",
    "one : 420 \n",
    "\n",
    "lorry : 313 \n",
    "\n",
    "will : 290 \n",
    "\n",
    "upon : 289 \n",
    "\n",
    "little : 264 \n",
    "\n",
    "man : 259 \n",
    "\n",
    "defarge : 259 \n",
    "\n",
    "time : 236\n",
    "\n",
    "* Note, at least \"said\" and \"mr\" seem to be common words. Feel free to add more to your stopwords file if you wish to get to less common words.\n",
    "Hints\n",
    "\n",
    "1. Which Data Structure? If you aren't sure which data structure to use, remember that we discussed a data structure this week that gives us a key and a value at that key (dictionaries). This could be really useful here.\n",
    "\n",
    "2.  Stripping off Punctuation.  The command \"replace\" on a string will replace one letter with another.  For example:\n",
    "\n",
    "word = word.replace(\".\",\"\")\n",
    "\n",
    "Will remove any periods from the word.\n",
    "\n",
    "3.  Sorting the data structure.  If you used an unordered data structure like a dictionary, you might need get the values out of it (into a list) to sort it.  You could also use \"collections.Counter\" to help with this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
